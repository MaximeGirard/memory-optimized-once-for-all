# TRAINING & GENERAL
args:
  path: "models/MC_OFA/teacher"
  t_path: "models/MC_OFA/teacher"
  teacher_path: "models/MC_OFA/teacher/"
  pretrained_model_path: "models/MC_OFA/"
  checkpoint: "models/MC_OFA/checkpoint/checkpoint-expand_2.pth.tar"
  model: "constant_V3"
  dataset: "imagenet"
  ks_list: [3, 5, 7]
  expand_list: [2, 3, 4]
  depth_list: [2, 3, 4]
  dynamic_batch_size: 1
  base_lr: 3.0e-3
  manual_seed: 0
  lr_schedule_type: "cosine"
  base_batch_size: 64
  valid_size: 100
  opt_type: "sgd"
  momentum: 0.9
  no_nesterov: false
  weight_decay: 3.0e-5
  label_smoothing: 0.1
  no_decay_keys: "bn#bias"
  fp16_allreduce: false
  model_init: "he_fout"
  validation_frequency: 1
  print_frequency: 10
  n_worker: 12
  resize_scale: 0.08
  distort_color: "tf"
  image_size: [128, 160, 192, 224]
  continuous_size: true
  not_sync_distributed_image_size: false
  bn_momentum: 0.1
  bn_eps: 1.0e-5
  dropout: 0.1
  base_stage_width: "proxyless"
  width_mult_list: 1.0
  dy_conv_scaling_mode: 1
  independent_distributed_sampling: false
  kd_ratio: 1.0
  kd_type: "ce"

args_per_task:
  kernel_1:
    n_epochs: 120
    base_lr: 3.0e-3
    warmup_epochs: 5
    warmup_lr: 3.0e-3
    task_ks_list: [3, 5, 7]
    task_expand_list: [4]
    task_depth_list: [4]
    dynamic_batch_size: 1
  depth_1:
    n_epochs: 25
    base_lr: 2.5e-4
    warmup_epochs: 0
    warmup_lr: 2.5e-3
    task_ks_list: [3, 5, 7]
    task_expand_list: [4]
    task_depth_list: [3, 4]
    dynamic_batch_size: 2
  depth_2:
    n_epochs: 120
    base_lr: 7.5e-4
    warmup_epochs: 5
    warmup_lr: 7.5e-5
    task_ks_list: [3, 5, 7]
    task_expand_list: [4]
    task_depth_list: [2, 3, 4]
    dynamic_batch_size: 2
  expand_1:
    n_epochs: 25
    base_lr: 2.5e-4
    warmup_epochs: 0
    warmup_lr: 2.5e-5
    task_ks_list: [3, 5, 7]
    task_expand_list: [3, 4]
    task_depth_list: [2, 3, 4]
    dynamic_batch_size: 4
  expand_2:
    n_epochs: 120
    base_lr: 2.5e-4
    warmup_epochs: 5
    warmup_lr: 7.5e-5
    task_ks_list: [3, 5, 7]
    task_expand_list: [2, 3, 4]
    task_depth_list: [2, 3, 4]
    dynamic_batch_size: 4

tasks: ["kernel", "depth", "expand"]
tasks_phases:
  kernel: [1]
  depth: [1, 2]
  expand: [1, 2]

# EVAL
subnet_config:
  random_sample: false
  ks: [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]
  e: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]
  d: [4, 4, 4, 4, 4]
  image_size: 224
  res_dir: "results/MC_OFA/"
  name: "reference_subnet"
  draw_graphs: true

# SEARCH
search_config:
  N_constraint: 1
  max_constraint: 350000
  min_constraint: 350000
  res_dir: "searches/MC_OFA/"
  acc_dataset_path: "acc_predictor_datasets/adapted/MC_OFA/dataset.pkl"
  acc_predictor_checkpoint: "acc_predictor_checkpoints/MC_OFA_acc.pth.tar"

# LOGGING
wandb:
  use_wandb: true
  project_name: "once-for-all-memory-constant"  
